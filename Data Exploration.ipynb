{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c017b948",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9407a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869773db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset doc2dial (./data_cache/doc2dial/dialogue_domain/1.0.1/c15afdf53780a8d6ebea7aec05384432195b356f879aa53a4ee39b740d520642)\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "dialogue_dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"dialogue_domain\",  # this is the name of the dataset for the second subtask, dialog generation\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bdce100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset doc2dial (./data_cache/doc2dial/document_domain/1.0.1/c15afdf53780a8d6ebea7aec05384432195b356f879aa53a4ee39b740d520642)\n"
     ]
    }
   ],
   "source": [
    "document_dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"document_domain\",  # this is the name of the dataset for the second subtask, dialog generation\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1219d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f019f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dataset[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4385243",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_domain = 'dmv'\n",
    "search_doc_id = 'Top 5 DMV Mistakes and How to Avoid Them#3_0'\n",
    "search_id_sp = ['6', '7']\n",
    "\n",
    "def text_from_spans(search_domain, search_doc_id, search_id_sp, document_dataset):\n",
    "    start = time.time()\n",
    "    total_answer = ''\n",
    "    for doc in document_dataset:\n",
    "        if doc['domain'] == search_domain and doc['doc_id'] == search_doc_id:\n",
    "            for span in doc['spans']:\n",
    "                if span['id_sp'] in search_id_sp:\n",
    "                    total_answer+=span['text_sp']\n",
    "            break\n",
    "    print(f\"Time elapsed: {time.time() - start}\")\n",
    "    return total_answer\n",
    "\n",
    "text_from_spans(search_domain, search_doc_id, search_id_sp, document_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d09bea",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b41ec8",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- [X] Sliding windows from the Document\n",
    "- [ ] Extract user utterance\n",
    "- [ ] Extract Dialogue history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67840f",
   "metadata": {},
   "source": [
    "### Sliding windows from the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec52a056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n",
      "0it [00:00, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "10it [00:03,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 3.1300320625305176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Defining train_dict\n",
    "train_dict = dict()\n",
    "train_dict['train_document'] = []\n",
    "train_dict['train_id_sp'] = []\n",
    "train_dict['train_user_utterance'] = []\n",
    "train_dict['train_doc_domain'] = []\n",
    "train_dict['train_doc_id'] = []\n",
    "train_dict['train_text_sp'] = []\n",
    "train_dict['train_dial_id_turn_id'] = []     # necessary for evaluation\n",
    "train_dict['train_start_pos'] = []     \n",
    "train_dict['train_end_pos'] = []     \n",
    "train_dict['train_start_tok'] = []     \n",
    "train_dict['train_end_tok'] = []  \n",
    "\n",
    "start = time.time()\n",
    "for idx, dialogue in tqdm(enumerate(dialogue_dataset)):\n",
    "    if idx == 10:\n",
    "        break\n",
    "    dial_id_turn_id = []       # running list of <dial_id>_<turn_id> for evaluation\n",
    "    sp_id_list = []            # running list of spans per document\n",
    "    user_utterance_list = []   # running list of user utterances per document\n",
    "    \n",
    "    for turn in dialogue['turns']:\n",
    "        dial_id_turn_id.append(dialogue['dial_id'] + '_' + str(turn['turn_id']))\n",
    "        if turn['role'] == 'user':\n",
    "            # TURN UTTERANCE IS FLATTENED AND ONLY THE [INPUT_IDS] IS STORED\n",
    "            turn['utterance'] = tokenizer(turn['utterance'], padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].view(-1)\n",
    "            user_utterance_list.append(turn['utterance'])   # adding user utterance to user_utterance_list\n",
    "        else:\n",
    "            references = turn['references']\n",
    "            ref_sp_id = []\n",
    "            for ref in references:\n",
    "                ref_sp_id.append(ref['sp_id'])\n",
    "            sp_id_list.append(ref_sp_id)          # adding list of sp_ids per dialogue to list of sp_ids per document\n",
    "    train_dict['train_id_sp'].append(sp_id_list)\n",
    "    train_dict['train_user_utterance'].append(user_utterance_list)\n",
    "    train_dict['train_doc_domain'].append(dialogue['domain'])\n",
    "    train_dict['train_doc_id'].append(dialogue['doc_id'])\n",
    "    train_dict['train_dial_id_turn_id'].append(dial_id_turn_id)\n",
    "    \n",
    "    for doc in document_dataset:\n",
    "        if doc['doc_id'] == train_dict['train_doc_id'][-1]:\n",
    "            # DOCUMENT TEXT IS NOT A TENSOR. PREVIOUSLY WE HAD tokenizer( )['index_ids'].view(-1)\n",
    "            doc['doc_text'] = tokenizer(doc['doc_text'], padding=True, truncation=False, return_tensors=\"pt\")\n",
    "            train_dict['train_document'].append(doc['doc_text'])          # adding the total document text\n",
    "            text_sp_2 = []            \n",
    "            start_sp_list = []         # big start sp list\n",
    "            end_sp_list = []           # big end sp list        \n",
    "            start_tok_list = []         # big start token list\n",
    "            end_tok_list = []           # big end token list     \n",
    "            for train_spans_id in train_dict['train_id_sp'][-1]:    \n",
    "                text_sp = \"\"         \n",
    "                ref_start_pos_list = []\n",
    "                ref_end_pos_list = []      \n",
    "                for span in doc['spans']:                    \n",
    "                    if span['id_sp'] in train_spans_id:\n",
    "                        text_sp += span['text_sp']                        \n",
    "                        ref_start_pos_list.append(span['start_sp'])\n",
    "                        ref_end_pos_list.append(span['end_sp'])    \n",
    "                start_pos = np.amin(ref_start_pos_list)\n",
    "                start_sp_list.append(start_pos)\n",
    "                # convert start_pos to start_token\n",
    "                start_tok_pos = doc['doc_text'].char_to_token(start_pos)\n",
    "                start_tok_list.append(start_tok_pos)\n",
    "                # convert end_pos to end_token\n",
    "                end_pos = np.amax(ref_end_pos_list)\n",
    "                end_sp_list.append(end_pos)\n",
    "                end_tok_pos = doc['doc_text'].char_to_token(end_pos)\n",
    "                end_tok_list.append(end_tok_pos)\n",
    "                text_sp_2.append(text_sp)\n",
    "            train_dict['train_text_sp'].append(text_sp_2)\n",
    "            train_dict['train_start_pos'].append(start_sp_list)\n",
    "            train_dict['train_end_pos'].append(end_sp_list)\n",
    "            train_dict['train_start_tok'].append(start_tok_list)\n",
    "            train_dict['train_end_tok'].append(end_tok_list)\n",
    "            break\n",
    "end = time.time()\n",
    "print(f'Total time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5fc1d",
   "metadata": {},
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User utterances:')\n",
    "print(train_dict['train_user_utterance'][0])\n",
    "\n",
    "print('\\nID Sp:')\n",
    "print(train_dict['train_id_sp'][0])\n",
    "\n",
    "print('\\nDoc ID:')\n",
    "print(train_dict['train_doc_id'][0])\n",
    "\n",
    "print('\\nDoc domain:')\n",
    "print(train_dict['train_doc_domain'][0])\n",
    "\n",
    "print('\\nTrain text spans:')\n",
    "print(train_dict['train_text_sp'][0])\n",
    "\n",
    "print('\\nDial_ID Turn_ID:')\n",
    "print(train_dict['train_dial_id_turn_id'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDoc text:')\n",
    "print(train_dict['train_document'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e18443",
   "metadata": {},
   "source": [
    "## Create a Dataframe out of the train_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211d7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\")\n",
    "print(metric.features) #this shows you what format the metric is expecting\n",
    "\n",
    "prediction = {'id': <rc dataset is of shape dialid_turnid - this value has to match the answer>,\n",
    "              'prediction_text': <your prediction>,\n",
    "              'no_answer_probability': 0.0} #edwin said we can ignore this for task 1\n",
    "reference = {'id': <see prediction>, \n",
    "              'answers': {\n",
    "                  'text': [list of answer, best to use the ones from the rc dataset],                                       \n",
    "                  'answer_start': [list of numbers of the answer star char again see rc dataset. ]}\n",
    "            }\n",
    "\n",
    "metric.add(prediction=prediction, reference=reference)\n",
    "final_score = metric.compute()\n",
    "final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f173f9",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fb18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc0b781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded question: [CLS] hello, i forgot o update my address, can you help me with that? [SEP]\n"
     ]
    }
   ],
   "source": [
    "question=train_dict['train_user_utterance'][0][0]\n",
    "print(f'Decoded question: {tokenizer.decode(question)}')\n",
    "# If already tokenized from dataset\n",
    "text=train_dict['train_document'][0]    # tokenized text\n",
    "# if simple text\n",
    "#text='By statute , you must report a change of address to DMV within ten days of moving. That is the case for the address associated with your license, as well as all the addresses associated with each registered vehicle, which may differ.'\n",
    "#text=tokenizer([text],  return_tensors=\"pt\")['input_ids'].view(-1)\n",
    "\n",
    "def text_mask(question, text):\n",
    "    '''   \n",
    "    text['input_ids'].view(-1)[1:]  was on the line below - need to do this to text\n",
    "    before using it\n",
    "    \n",
    "    input_ids: will be the question and the window of the document concat together\n",
    "    segment_ids: is a mask that makes the two sentences distinct 1's for question 0 for document text\n",
    "    '''\n",
    "    \n",
    "    input_ids=torch.cat((question, text), 0)\n",
    "    SEP_token_id=102\n",
    "    sep_idx = (input_ids == 102).nonzero(as_tuple=False)[0][0].item()\n",
    "    num_seg_a = sep_idx+1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    return input_ids, segment_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13828a3",
   "metadata": {},
   "source": [
    "Create mask for start and end positions. This way we only check the first token after the '.' as start positions, and the tokens before the '.' as end positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbff54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token id for '.' = 1012\n",
    "def mask_start_end(input_ids_trunc, segment_ids_trunc, mode):\n",
    "    \"\"\"Returns a mask for the start and end logits. \n",
    "    input_ids_trunc = tokens (tensor)\n",
    "    segment_ids_trunc = mask (question / text)\n",
    "    mode = \"start\" or 'end'\n",
    "    return tensor\n",
    "    \"\"\"\n",
    "    a = torch.where(input_ids_trunc == 1012, 1, 0)   # mask=1 for '.'\n",
    "    a = a * torch.tensor(segment_ids_trunc)          # mask question - text\n",
    "    if mode=='start':\n",
    "        b = torch.cat((torch.tensor([0]),a),0)[:-1]     # move the 1s one position to the right\n",
    "    else:\n",
    "        b = torch.cat((a, torch.tensor([0])),0)[1:]\n",
    "    assert len (a) == len(b)\n",
    "    return b\n",
    "\n",
    "def tensor_to_positive(tensor, mask):\n",
    "    \"\"\" All the values need to be higher than 0, since 0s are values for the mask\n",
    "    and we don't want to choose them when selecting the start or end token.\n",
    "    Return torch.tensor \"\"\"\n",
    "    min_value = torch.amin(tensor) \n",
    "    tensor_positive = tensor + (mask * np.abs(min_value.detach().numpy()))\n",
    "    return tensor_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f45a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "[CLS] hello, i forgot o update my address, can you help me with that? [SEP]\n",
      "\n",
      "Answer:\n",
      "forgetting to update address by statute , you must report a change of address to d ##m ##v within ten days of moving.\n"
     ]
    }
   ],
   "source": [
    "input_ids_trunc = input_ids[:511]\n",
    "input_ids_trunc = torch.cat((input_ids_trunc, torch.tensor([102])),0)\n",
    "segment_ids_trunc = segment_ids[:512]\n",
    "\n",
    "output = model(input_ids_trunc.view(1,-1), token_type_ids=torch.tensor([segment_ids_trunc]))\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_trunc)   # table with token_id -> word\n",
    "#tokens with highest start and end scores\n",
    "mask_start = mask_start_end(input_ids_trunc, segment_ids_trunc, 'start')\n",
    "start_logits_positive = tensor_to_positive(output.start_logits * mask_start, mask_start)\n",
    "answer_start = torch.argmax(start_logits_positive)  # token index for the highest start token\n",
    "max_start_prob = output.start_logits[0][answer_start].item()\n",
    "mask_end = mask_start_end(input_ids_trunc, segment_ids_trunc, 'end')\n",
    "end_logits_positive = tensor_to_positive(output.end_logits * mask_end, mask_end)\n",
    "answer_end = torch.argmax(end_logits_positive)\n",
    "max_end_prob = output.end_logits[0][answer_end].item()\n",
    "sum_joint_prob = max_start_prob + max_end_prob\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "    \n",
    "print(\"\\nQuestion:\\n{}\".format(tokenizer.decode(question)))\n",
    "print(\"\\nAnswer:\\n{}.\".format(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99488922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  7592,  1010,  1045,  9471,  1051, 10651,  2026,  4769,  1010])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cfc45ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6149070858955383 1.1371694803237915 0.5222623944282532\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce30b584",
   "metadata": {},
   "source": [
    "### Output - For Report\n",
    "This section of text shows that span [49][50][51][52] is what we return. However, the section belows is what the ground truth says. SPan [51] is highlighted in red (we return 51, ground truth doesn't contain it).\n",
    "\n",
    "- 'About ten percent of customers visiting a DMV office do not bring what they need to complete their transaction, and have to come back a second time to finish their business. This can be as simple as not bringing sufficient funds to pay for a license renewal or not having the proof of auto insurance required to register a car. <font color='red'>Better yet ,</font> don t visit a DMV office at all, and see if your transaction can be performed online, like an address change, registration renewal, license renewal, replacing a lost title, paying a DRA or scheduling a road test. '\n",
    "- 'About ten percent of customers visiting a DMV office do not bring what they need to complete their transaction, and have to come back a second time to finish their business. This can be as simple as not bringing sufficient funds to pay for a license renewal or not having the proof of auto insurance required to register a car. don t visit a DMV office at all, and see if your transaction can be performed online, like an address change, registration renewal, license renewal, replacing a lost title, paying a DRA or scheduling a road test. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92eb3a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.2159, -7.2462, -8.1272, -6.2053, -6.5941, -7.5722, -8.0030, -7.5578,\n",
       "         -8.1340, -8.1911, -6.6241, -7.2704, -7.2502, -7.8474, -7.6332, -8.7230,\n",
       "         -9.2755, -0.2159, -7.0171, -5.2435, -8.2287, -7.9156, -6.8040, -7.8129,\n",
       "         -7.5445, -6.2596, -8.8288, -5.3737, -8.7227, -7.9145, -8.7598, -7.2496,\n",
       "         -6.9781, -8.9276, -7.9391, -7.4602, -8.5356, -6.6556, -8.0144, -8.8903,\n",
       "         -7.0341, -8.2031, -7.0752, -8.9533, -7.5368, -7.2984, -8.2308, -7.3852,\n",
       "         -8.3005, -8.5413, -6.9700, -7.9976, -9.0184, -8.2447, -8.5973, -8.8123,\n",
       "         -6.4067, -8.2487, -7.4378, -7.0112, -6.6630, -8.2115, -7.7322, -7.3236,\n",
       "         -8.0114, -6.5850, -8.4931, -8.6287, -7.1558, -8.5910, -6.8763, -7.8212,\n",
       "         -7.3972, -8.5983, -6.6899, -1.7091, -5.8911, -0.6149, -5.8702, -4.5563,\n",
       "         -3.8890, -1.4947, -2.7303, -5.3168,  0.5116, -1.0279, -0.8370, -4.4708,\n",
       "         -1.6109, -7.5000, -4.0569, -5.7624, -1.7996, -6.6634, -5.7550, -4.4357,\n",
       "         -4.6125, -6.0911, -8.3443, -5.2019, -5.5797, -3.6543, -7.0543, -5.5553,\n",
       "         -6.2548, -6.5757, -3.6928, -2.5298, -5.6451, -7.1314, -3.1890, -4.0500,\n",
       "         -7.7916, -6.6441, -7.5862, -7.9879, -3.9240, -7.1534, -4.1437, -7.2828,\n",
       "         -8.6766, -6.5706, -5.6562, -6.2040, -8.2655, -7.1940, -5.3386, -5.6355,\n",
       "         -6.7850, -2.8027, -7.8354, -5.5205, -6.2050, -7.6417, -8.0288, -8.2548,\n",
       "         -3.4155, -6.7436, -5.9293, -6.0751, -7.0663, -7.0620, -6.8838, -8.2665,\n",
       "         -6.3240, -6.9917, -6.3936, -8.5686, -3.1038, -6.2434, -4.6093, -7.5703,\n",
       "         -5.8295, -7.3089, -8.3821, -8.3874, -3.9467, -7.3050, -6.7441, -7.5918,\n",
       "         -7.0404, -7.9819, -8.1592, -6.2182, -6.5460, -2.1133, -5.1061, -5.0104,\n",
       "         -7.5017, -5.8718, -6.2864, -5.6707, -6.4975, -7.9199, -2.8124, -5.4364,\n",
       "         -4.4401, -7.3128, -5.3373, -6.6933, -8.6732, -5.2886, -6.6880, -5.7794,\n",
       "         -8.1565, -6.1957, -8.2182, -6.7785, -8.5716, -8.3169, -8.7770, -6.9746,\n",
       "         -9.1978, -8.6812, -8.9738, -6.7192, -6.9989, -8.5707, -8.7051, -7.0230,\n",
       "         -9.2073, -6.8871, -6.3139, -8.4141, -8.8378, -7.9403, -8.6431, -8.4545,\n",
       "         -6.4090, -7.1909, -7.1387, -8.7079, -7.0317, -5.1489, -5.7254, -7.6639,\n",
       "         -9.0645, -4.5976, -7.9109, -5.7753, -5.6000, -8.4586, -7.6082, -5.5888,\n",
       "         -6.4572, -7.6334, -7.4678, -6.5637, -7.8970, -8.5219, -6.5552, -7.6404,\n",
       "         -3.8599, -8.4243, -6.9819, -8.2136, -7.5783, -6.9877, -8.9199, -6.7796,\n",
       "         -8.9293, -7.6404, -8.0418, -9.0050, -5.8340, -8.3374, -6.4114, -8.7432,\n",
       "         -8.8437, -8.5978, -7.7183, -8.8304, -6.9660, -8.7868, -8.9347, -8.6033,\n",
       "         -8.4324, -9.1544, -8.9947, -9.0884, -9.1785, -8.0091, -7.7225, -8.4686,\n",
       "         -7.8669, -8.4296, -8.3488, -8.8588, -9.0140, -7.0102, -8.2525, -8.3703,\n",
       "         -7.6678, -9.0338, -8.4944, -8.4169, -8.6344, -8.9273, -8.1681, -6.8732,\n",
       "         -8.5683, -9.0061, -8.4370, -9.0648, -8.5710, -7.9947, -8.3985, -8.7691,\n",
       "         -7.3058, -8.3866, -7.8907, -8.6699, -8.5881, -9.1521, -8.4544, -9.1333,\n",
       "         -7.8183, -9.1429, -8.9871, -8.2499, -8.7789, -9.1722, -9.0960, -8.6095,\n",
       "         -6.8622, -8.7033, -8.4673, -8.2745, -8.4226, -9.1548, -7.7811, -9.2110,\n",
       "         -8.6084, -8.8193, -8.8604, -8.8637, -8.4506, -8.4808, -8.5605, -9.1773,\n",
       "         -7.6854, -7.2537, -7.5894, -7.8980, -8.8723, -8.1969, -8.9565, -8.5800,\n",
       "         -7.4639, -7.9106, -8.3549, -7.5794, -8.1117, -8.4745, -8.8628, -8.4441,\n",
       "         -8.4232, -8.4859, -8.8964, -8.8975, -8.8795, -8.7841, -8.0653, -7.6967,\n",
       "         -9.0831, -7.8244, -8.9301, -8.0844, -8.1671, -6.6820, -8.0498, -6.0521,\n",
       "         -8.4306, -8.4364, -8.5441, -7.6686, -8.2507, -8.1979, -8.4527, -8.8118,\n",
       "         -7.4878, -8.2050, -7.5900, -8.1079, -8.6718, -8.6518, -8.7958, -7.9753,\n",
       "         -8.7187, -8.9842, -7.1141, -8.8915, -8.6864, -8.6437, -8.9818, -8.1220,\n",
       "         -8.5122, -7.8055, -8.6704, -7.7105, -8.3562, -9.1321, -7.6462, -7.5089,\n",
       "         -8.5984, -6.5375, -8.3194, -8.4773, -9.0040, -7.5519, -8.5043, -8.7564,\n",
       "         -8.7382, -8.9573, -8.8273, -8.1849, -8.8393, -8.3146, -9.0513, -9.2095,\n",
       "         -8.5931, -9.1500, -9.0059, -6.3861, -8.5391, -8.6504, -7.9029, -8.2123,\n",
       "         -8.3040, -9.1602, -8.3634, -8.0188, -8.5482, -8.3433, -8.2026, -8.9398,\n",
       "         -8.7370, -8.8060, -8.5797, -9.0421, -8.3816, -8.3672, -5.7084, -8.1167,\n",
       "         -8.4224, -7.6444, -8.5264, -7.0384, -8.1822, -8.2218, -7.2289, -7.5992,\n",
       "         -8.4042, -7.8265, -8.0883, -8.4503, -8.0766, -6.8285, -7.4782, -7.8938,\n",
       "         -7.4904, -8.6522, -8.0699, -8.7067, -8.4250, -8.1328, -8.3918, -6.9863,\n",
       "         -8.6764, -8.6950, -8.5370, -6.1019, -6.4568, -8.4455, -7.1969, -7.0065,\n",
       "         -7.0134, -7.3394, -8.3610, -8.0020, -8.3623, -8.0887, -7.0309, -8.7200,\n",
       "         -8.3491, -7.7849, -7.7550, -6.8147, -7.9527, -7.7536, -8.9535, -8.8015,\n",
       "         -7.8364, -8.1930, -8.6077, -8.7716, -7.2053, -6.0088, -7.8171, -7.2915,\n",
       "         -8.2869, -8.2595, -8.3575, -8.1900, -8.5069, -8.4132, -8.8952, -5.8947,\n",
       "         -7.8912, -5.2832, -8.3569, -7.8526, -8.0112, -8.7582, -8.4395, -6.1607,\n",
       "         -8.3642, -8.0016, -9.3105, -8.8929, -6.0237, -6.5211, -7.3044, -7.1547,\n",
       "         -4.6596, -5.3795, -7.7615, -8.9641, -8.3861, -7.5898, -8.7737, -0.2157]],\n",
       "       grad_fn=<CloneBackward0>), end_logits=tensor([[ 0.4502, -7.0734, -7.1547, -6.9672, -7.1175, -7.2961, -7.6874, -7.4878,\n",
       "         -6.2254, -6.2914, -5.8205, -5.9090, -6.8250, -7.1800, -7.6524, -7.1258,\n",
       "         -7.3834,  0.4502, -7.9676, -7.6627, -7.8285, -5.4793, -5.5845, -7.7793,\n",
       "         -8.3946, -7.4656, -7.4862, -4.0960, -7.8816, -8.3115, -7.7021, -8.3704,\n",
       "         -5.2340, -5.8512, -8.4807, -7.6032, -8.2567, -7.9462, -5.7328, -8.2593,\n",
       "         -8.6687, -7.8718, -5.3158, -7.2745, -8.4784, -7.7100, -8.1900, -6.9691,\n",
       "         -8.0868, -8.3419, -6.3781, -8.0646, -8.2057, -8.2553, -6.5422, -6.9296,\n",
       "         -7.8021, -7.9269, -8.1219, -8.3243, -6.2864, -8.3429, -8.6680, -8.3664,\n",
       "         -7.3483, -8.4803, -8.3474, -7.8374, -5.7733, -8.3526, -8.3404, -8.5994,\n",
       "         -7.6107, -6.2656, -5.5276, -3.0835, -4.9375, -2.0491, -5.2355, -2.8116,\n",
       "         -1.3624, -6.3862, -0.6468, -2.8100, -5.6166, -5.3510, -4.2548, -6.8939,\n",
       "         -3.7322, -6.4381,  0.5149, -6.1487, -5.3363, -5.8964,  1.4725, -6.6995,\n",
       "         -6.3528, -3.4802, -6.7289,  1.1372, -0.1012, -6.5671, -7.9905, -8.5141,\n",
       "         -5.2426, -7.7138, -7.5174, -3.7797, -6.3371, -7.6425, -6.9141, -1.8428,\n",
       "         -3.2587, -8.1343, -6.9037, -7.3849, -7.1108, -7.7529, -4.3947, -5.7517,\n",
       "         -7.3655, -6.8809, -6.7417, -1.8858, -3.4883, -8.0246, -7.3636, -2.1287,\n",
       "         -2.8922, -7.7321, -8.2568, -7.4925, -6.0070, -8.2140, -7.9044, -7.7836,\n",
       "         -6.4429, -8.5514, -6.8064, -3.9811, -8.1239, -7.5858, -5.9702, -8.0425,\n",
       "         -8.2363, -6.5642, -2.5744, -5.8481, -6.5889, -7.8815, -7.2991, -6.8382,\n",
       "         -6.0534, -2.1988, -5.3315, -7.7583, -6.4921, -8.2086, -6.0572, -3.1017,\n",
       "         -7.1242, -7.2987, -7.4271, -1.5133, -2.4086, -7.4896, -8.0372, -7.2778,\n",
       "         -7.9676, -7.6898, -7.9612, -4.5569, -2.8492, -4.1481, -6.7755, -7.5831,\n",
       "         -5.6508, -6.9353, -4.0154, -0.6376, -6.3843, -6.9922, -7.1552, -4.6900,\n",
       "         -6.2859, -5.7283, -7.3120, -7.4581, -7.7874, -7.1901, -6.2112, -1.7579,\n",
       "         -7.7760, -7.7353, -7.4556, -7.0621, -7.6648, -7.7271, -6.7999, -1.4062,\n",
       "         -4.6872, -7.4361, -8.2050, -8.2658, -8.3850, -5.7171, -3.2348, -6.0497,\n",
       "         -7.6740, -7.8546, -3.9346, -5.3097, -8.1480, -8.5191, -8.1998, -7.2195,\n",
       "         -8.1356, -7.4652, -8.4581, -8.6680, -6.0307, -8.4923, -8.8660, -7.4294,\n",
       "         -6.7858, -2.5106, -5.6663, -7.7119, -7.8676, -8.7838, -8.4850, -8.7846,\n",
       "         -6.3926, -8.4706, -4.3807, -8.5939, -8.5409, -5.6891, -8.6073, -7.4828,\n",
       "         -5.0730, -8.3178, -7.3265, -6.4161, -6.2870, -6.7742, -8.3457, -8.5607,\n",
       "         -7.4847, -8.0279, -8.4952, -7.5213, -8.0852, -7.9839, -6.1100, -7.0890,\n",
       "         -7.2135, -8.3083, -7.8862, -5.6785, -6.1282, -7.7831, -8.8394, -8.6205,\n",
       "         -8.2890, -8.4926, -8.3391, -6.7330, -7.3571, -8.4485, -8.2097, -8.5931,\n",
       "         -8.6757, -8.1832, -8.3823, -8.6142, -7.1260, -8.3436, -8.5762, -8.3071,\n",
       "         -7.6447, -7.8223, -7.0695, -8.4414, -6.1418, -5.4761, -7.9681, -8.1076,\n",
       "         -8.3244, -8.4349, -8.6443, -8.4625, -8.3203, -8.2974, -6.5822, -7.6830,\n",
       "         -8.2641, -7.6564, -8.5052, -8.5337, -7.8344, -6.6551, -6.1745, -8.0559,\n",
       "         -8.5001, -8.4801, -8.4420, -8.1851, -6.4416, -8.1151, -8.2838, -6.5610,\n",
       "         -8.4646, -8.3430, -8.3616, -8.3420, -8.4645, -8.1936, -8.0471, -6.4478,\n",
       "         -5.5887, -8.4810, -8.1510, -7.3316, -8.4314, -8.3383, -6.8582, -7.3946,\n",
       "         -8.3581, -8.3885, -8.4157, -8.6048, -8.3768, -8.3024, -7.8203, -8.3986,\n",
       "         -7.8677, -8.1541, -7.9896, -8.1258, -7.1962, -5.5445, -5.6213, -7.3645,\n",
       "         -6.9708, -8.4945, -8.2377, -8.2728, -8.3792, -8.5015, -7.7023, -7.9846,\n",
       "         -7.9163, -5.6454, -8.0475, -8.3758, -7.8527, -5.8436, -8.1742, -8.2172,\n",
       "         -8.3760, -8.1094, -7.3571, -6.3833, -8.0520, -7.8348, -5.7723, -5.2403,\n",
       "         -7.4730, -7.5506, -8.2657, -8.1654, -8.3928, -6.8403, -8.4944, -8.0911,\n",
       "         -8.5017, -8.2753, -8.4792, -7.7833, -7.7753, -6.1611, -5.5210, -6.8153,\n",
       "         -7.1605, -7.4823, -7.5256, -7.5094, -5.8499, -8.0225, -8.1726, -8.1975,\n",
       "         -8.1086, -7.6463, -8.2664, -7.4679, -8.1301, -8.3642, -8.1406, -7.7603,\n",
       "         -8.2600, -6.5172, -7.1798, -8.1188, -7.1055, -7.0990, -8.0005, -8.2076,\n",
       "         -8.3004, -6.9090, -8.1506, -7.7681, -7.9716, -5.9734, -8.1156, -7.9868,\n",
       "         -7.4302, -8.0231, -7.6519, -8.0991, -5.9558, -6.0514, -7.8249, -7.8157,\n",
       "         -6.4020, -7.7433, -7.9022, -7.0327, -5.9228, -8.0404, -7.0773, -7.0005,\n",
       "         -7.9514, -6.9141, -5.1852, -5.2351, -7.3378, -7.6239, -7.6939, -8.2531,\n",
       "         -7.6230, -8.1260, -6.0356, -8.1772, -7.8784, -5.4557, -6.0596, -8.2347,\n",
       "         -8.0172, -6.7599, -6.9879, -7.2837, -7.0475, -7.1714, -7.6873, -6.8963,\n",
       "         -6.7175, -4.4940, -7.6765, -8.0169, -7.4929, -8.2417, -7.9282, -7.6640,\n",
       "         -7.8618, -4.9179, -4.9373, -7.4710, -8.1056, -8.2576, -8.1205, -7.5937,\n",
       "         -7.4850, -6.3173, -8.0085, -6.0090, -5.4683, -8.3964, -8.5408, -8.1077,\n",
       "         -7.3104, -8.5126, -7.9154, -8.3218, -8.4425, -6.5448, -7.2924, -7.8606,\n",
       "         -8.3031, -6.3166, -8.1909, -7.7313, -5.0561, -7.3756, -6.3592, -8.6382,\n",
       "         -8.4461, -8.3174, -7.3781, -6.7269, -7.8781, -7.3898, -4.4670, -4.5762,\n",
       "         -7.5085, -6.5015, -7.9549, -7.9567, -6.9567, -7.3541, -6.0445,  0.4501]],\n",
       "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfaf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(question, document, slide=256):\n",
    "#     input_ids is the document\n",
    "    start = 0\n",
    "    end = 512 - question.size()[0]\n",
    "    windows = []\n",
    "    doc_size = len(td)\n",
    "    while(start <= doc_size):\n",
    "        print(start, end, doc_size)\n",
    "        windows.append(td[start:end])\n",
    "\n",
    "        start += slide\n",
    "        # if there are less tokens than the slide amount\n",
    "        if (doc_size - start) < slide:\n",
    "            print(\"hello\")\n",
    "            end = doc_size\n",
    "        else:\n",
    "            if slide < doc_size - end:\n",
    "                slide = doc_size - end\n",
    "            end += slide\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b07c048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello, i forgot o update my address, can you help me with that? [SEP]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in windows:\n",
    "    w = \" \".join(window)\n",
    "    print(w)\n",
    "    print('----------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98636b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  7592,  1010,  1045,  9471,  1051, 10651,  2026,  4769,  1010,\n",
       "         2064,  2017,  2393,  2033,  2007,  2008,  1029,   102])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17791720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving for if I ruin it\n",
    "\n",
    "question=train_dict['train_user_utterance'][0][0]\n",
    "print(f'Decoded question: {tokenizer.decode(question)}')\n",
    "# If already tokenized from dataset\n",
    "text=train_dict['train_document'][0]    # tokenized text\n",
    "# if simple text\n",
    "#text='By statute , you must report a change of address to DMV within ten days of moving. That is the case for the address associated with your license, as well as all the addresses associated with each registered vehicle, which may differ.'\n",
    "#text=tokenizer([text],  return_tensors=\"pt\")['input_ids'].view(-1)\n",
    "\n",
    "input_ids=torch.cat((question, text['input_ids'].view(-1)[1:]), 0)\n",
    "SEP_token_id=102\n",
    "sep_idx = (input_ids == 102).nonzero(as_tuple=False)[0][0].item()\n",
    "num_seg_a = sep_idx+1\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
