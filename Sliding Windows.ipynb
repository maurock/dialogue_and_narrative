{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c017b948",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9407a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869773db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset doc2dial (./data_cache/doc2dial/dialogue_domain/1.0.1/c15afdf53780a8d6ebea7aec05384432195b356f879aa53a4ee39b740d520642)\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "dialogue_dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"dialogue_domain\",  # this is the name of the dataset for the second subtask, dialog generation\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bdce100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset doc2dial (./data_cache/doc2dial/document_domain/1.0.1/c15afdf53780a8d6ebea7aec05384432195b356f879aa53a4ee39b740d520642)\n"
     ]
    }
   ],
   "source": [
    "document_dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"document_domain\",  # this is the name of the dataset for the second subtask, dialog generation\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d09bea",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b41ec8",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- [X] Sliding windows from the Document\n",
    "- [ ] Extract user utterance\n",
    "- [ ] Extract Dialogue history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67840f",
   "metadata": {},
   "source": [
    "### Sliding windows from the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec52a056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n",
      "0it [00:00, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "10it [00:03,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 3.049872875213623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Defining train_dict\n",
    "train_dict = dict()\n",
    "train_dict['train_document'] = []\n",
    "train_dict['train_id_sp'] = []\n",
    "train_dict['train_user_utterance'] = []\n",
    "train_dict['train_doc_domain'] = []\n",
    "train_dict['train_doc_id'] = []\n",
    "train_dict['train_text_sp'] = []\n",
    "train_dict['train_dial_id_turn_id'] = []     # necessary for evaluation\n",
    "train_dict['train_start_pos'] = []     \n",
    "train_dict['train_end_pos'] = []     \n",
    "train_dict['train_start_tok'] = []     \n",
    "train_dict['train_end_tok'] = []  \n",
    "\n",
    "start = time.time()\n",
    "for idx, dialogue in tqdm(enumerate(dialogue_dataset)):\n",
    "    if idx == 10:\n",
    "        break\n",
    "    dial_id_turn_id = []       # running list of <dial_id>_<turn_id> for evaluation\n",
    "    sp_id_list = []            # running list of spans per document\n",
    "    user_utterance_list = []   # running list of user utterances per document\n",
    "    \n",
    "    for turn in dialogue['turns']:\n",
    "        dial_id_turn_id.append(dialogue['dial_id'] + '_' + str(turn['turn_id']))\n",
    "        if turn['role'] == 'user':\n",
    "            # TURN UTTERANCE IS FLATTENED AND ONLY THE [INPUT_IDS] IS STORED\n",
    "            turn['utterance'] = tokenizer(turn['utterance'], padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].view(-1)\n",
    "            user_utterance_list.append(turn['utterance'])   # adding user utterance to user_utterance_list\n",
    "        else:\n",
    "            references = turn['references']\n",
    "            ref_sp_id = []\n",
    "            for ref in references:\n",
    "                ref_sp_id.append(ref['sp_id'])\n",
    "            sp_id_list.append(ref_sp_id)          # adding list of sp_ids per dialogue to list of sp_ids per document\n",
    "    train_dict['train_id_sp'].append(sp_id_list)\n",
    "    train_dict['train_user_utterance'].append(user_utterance_list)\n",
    "    train_dict['train_doc_domain'].append(dialogue['domain'])\n",
    "    train_dict['train_doc_id'].append(dialogue['doc_id'])\n",
    "    train_dict['train_dial_id_turn_id'].append(dial_id_turn_id)\n",
    "    \n",
    "    for doc in document_dataset:\n",
    "        if doc['doc_id'] == train_dict['train_doc_id'][-1]:\n",
    "            # DOCUMENT TEXT IS NOT A TENSOR. PREVIOUSLY WE HAD tokenizer( )['index_ids'].view(-1)\n",
    "            doc['doc_text'] = tokenizer(doc['doc_text'], padding=True, truncation=False, return_tensors=\"pt\")\n",
    "            train_dict['train_document'].append(doc['doc_text'])          # adding the total document text\n",
    "            text_sp_2 = []            \n",
    "            start_sp_list = []         # big start sp list\n",
    "            end_sp_list = []           # big end sp list        \n",
    "            start_tok_list = []         # big start token list\n",
    "            end_tok_list = []           # big end token list     \n",
    "            for train_spans_id in train_dict['train_id_sp'][-1]:    \n",
    "                text_sp = \"\"         \n",
    "                ref_start_pos_list = []\n",
    "                ref_end_pos_list = []      \n",
    "                for span in doc['spans']:                    \n",
    "                    if span['id_sp'] in train_spans_id:\n",
    "                        text_sp += span['text_sp']                        \n",
    "                        ref_start_pos_list.append(span['start_sp'])\n",
    "                        ref_end_pos_list.append(span['end_sp'])    \n",
    "                start_pos = np.amin(ref_start_pos_list)\n",
    "                start_sp_list.append(start_pos)\n",
    "                # convert start_pos to start_token\n",
    "                start_tok_pos = doc['doc_text'].char_to_token(start_pos)\n",
    "                start_tok_list.append(start_tok_pos)\n",
    "                # convert end_pos to end_token\n",
    "                end_pos = np.amax(ref_end_pos_list)\n",
    "                end_sp_list.append(end_pos)\n",
    "                end_tok_pos = doc['doc_text'].char_to_token(end_pos)\n",
    "                end_tok_list.append(end_tok_pos)\n",
    "                text_sp_2.append(text_sp)\n",
    "            train_dict['train_text_sp'].append(text_sp_2)\n",
    "            train_dict['train_start_pos'].append(start_sp_list)\n",
    "            train_dict['train_end_pos'].append(end_sp_list)\n",
    "            train_dict['train_start_tok'].append(start_tok_list)\n",
    "            train_dict['train_end_tok'].append(end_tok_list)\n",
    "            break\n",
    "end = time.time()\n",
    "print(f'Total time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5fc1d",
   "metadata": {},
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f173f9",
   "metadata": {},
   "source": [
    "# Functions for Dataset / Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc0b781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_mask(question, text):\n",
    "    '''   \n",
    "    text['input_ids'].view(-1)[1:] was on the line below where 'text' is now - \n",
    "    need to do this to text before sending it into this function\n",
    "    \n",
    "    input_ids: will be the question and the window of the document concat together\n",
    "    segment_ids: is a mask that makes the two sentences distinct 1's for question 0 for document text\n",
    "    '''\n",
    "    input_ids=torch.cat((question, text), 0)\n",
    "    SEP_token_id=102\n",
    "    sep_idx = (input_ids == 102).nonzero(as_tuple=False)[0][0].item()\n",
    "    num_seg_a = sep_idx+1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    return input_ids, segment_ids\n",
    "\n",
    "def add_sep_tokens(windows):\n",
    "    tmp = []\n",
    "    sep_token = 102\n",
    "    for window in windows:\n",
    "        end = len(window) - 1\n",
    "        if window[end] != sep_token:\n",
    "            # add a SEP token at the end\n",
    "            tmp.append(torch.cat((window, torch.tensor([102])),0))\n",
    "        else:\n",
    "            tmp.append(window)\n",
    "    return tmp\n",
    "\n",
    "def sliding_windows(question, document, stride=256):\n",
    "    # tokenized input_ids is the document - remove [CLS] before sending through\n",
    "    windows = []\n",
    "    model_tok_limit = 511  # model can take 512 tokens maximum - -1 to add a sep token at end of each window\n",
    "    start = 0\n",
    "    end = model_tok_limit - len(question)\n",
    "    doc_size = len(document)\n",
    "        \n",
    "    # handling edge case of documents smaller than models input (512 tokens)   \n",
    "    if len(document) <= model_tok_limit:\n",
    "        end = len(document)\n",
    "    \n",
    "    while(start <= doc_size):\n",
    "        # print(start, end, doc_size)\n",
    "        window = document[start:end]\n",
    "        windows.append(window)\n",
    "        \n",
    "        if end == doc_size: \n",
    "            break\n",
    "        \n",
    "        start += stride\n",
    "        # if there are less tokens than the slide amount\n",
    "        if (doc_size - (start + stride)) < stride:\n",
    "            end = doc_size\n",
    "        else:\n",
    "            end += stride\n",
    "    \n",
    "    windows = add_sep_tokens(windows)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13828a3",
   "metadata": {},
   "source": [
    "Create mask for start and end positions. This way we only check the first token after the '.' as start positions, and the tokens before the '.' as end positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49803a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded question: [CLS] hello, i forgot o update my address, can you help me with that? [SEP]\n"
     ]
    }
   ],
   "source": [
    "question=train_dict['train_user_utterance'][0][0]\n",
    "print(f'Decoded question: {tokenizer.decode(question)}')\n",
    "# If already tokenized from dataset\n",
    "text=train_dict['train_document'][0]    # tokenized text\n",
    "# if simple text\n",
    "#text='By statute , you must report a change of address to DMV within ten days of moving. That is the case for the address associated with your license, as well as all the addresses associated with each registered vehicle, which may differ.'\n",
    "#text=tokenizer([text],  return_tensors=\"pt\")['input_ids'].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b07c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Sliding Window [1:] to remove the [CLS] that was put in by the tokenizer\n",
    "    The Model likes '[CLS] Sentence1 [SEP] Sentence2 [SEP]' it doesn't need the [CLS]\n",
    "'''\n",
    "windows = sliding_windows(question, text['input_ids'][0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78260b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n",
      "439\n"
     ]
    }
   ],
   "source": [
    "for window in windows:\n",
    "    print(len(window) + len(question))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = []\n",
    "\n",
    "for window in windows:\n",
    "    model_inputs.append(text_mask(question, window))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d76782",
   "metadata": {},
   "source": [
    "## Got each window into the format:\n",
    "[CLS] question [SEP] doc_text [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cad9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_input in model_inputs:\n",
    "    print(tokenizer.decode(model_input[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e701b3",
   "metadata": {},
   "source": [
    "# This is where I have got too - Rest does not run yet\n",
    "\n",
    "## Our Model - BertForQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2837c",
   "metadata": {},
   "source": [
    "## Functions for after the model has been run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token id for '.' = 1012\n",
    "def mask_start_end(input_ids_trunc, segment_ids_trunc, mode):\n",
    "    \"\"\"Returns a mask for the start and end logits. \n",
    "    input_ids_trunc = tokens (tensor)\n",
    "    segment_ids_trunc = mask (question / text)\n",
    "    mode = \"start\" or 'end'\n",
    "    return tensor\n",
    "    \"\"\"\n",
    "    a = torch.where(input_ids_trunc == 1012, 1, 0)   # mask=1 for '.'\n",
    "    a = a * torch.tensor(segment_ids_trunc)          # mask question - text\n",
    "    if mode=='start':\n",
    "        b = torch.cat((torch.tensor([0]),a),0)[:-1]     # move the 1s one position to the right\n",
    "    else:\n",
    "        b = torch.cat((a, torch.tensor([0])),0)[1:]\n",
    "    assert len (a) == len(b)\n",
    "    return b\n",
    "\n",
    "def tensor_to_positive(tensor, mask):\n",
    "    \"\"\" All the values need to be higher than 0, since 0s are values for the mask\n",
    "    and we don't want to choose them when selecting the start or end token.\n",
    "    Return torch.tensor \"\"\"\n",
    "    min_value = torch.amin(tensor) \n",
    "    tensor_positive = tensor + (mask * np.abs(min_value.detach().numpy()))\n",
    "    return tensor_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae82c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_trunc = input_ids[:511]\n",
    "input_ids_trunc = torch.cat((input_ids_trunc, torch.tensor([102])),0)\n",
    "segment_ids_trunc = segment_ids[:512]\n",
    "\n",
    "output = model(input_ids_trunc.view(1,-1), token_type_ids=torch.tensor([segment_ids_trunc]))\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_trunc)   # table with token_id -> word\n",
    "#tokens with highest start and end scores\n",
    "mask_start = mask_start_end(input_ids_trunc, segment_ids_trunc, 'start')\n",
    "start_logits_positive = tensor_to_positive(output.start_logits * mask_start, mask_start)\n",
    "answer_start = torch.argmax(start_logits_positive)  # token index for the highest start token\n",
    "max_start_prob = output.start_logits[0][answer_start].item()\n",
    "mask_end = mask_start_end(input_ids_trunc, segment_ids_trunc, 'end')\n",
    "end_logits_positive = tensor_to_positive(output.end_logits * mask_end, mask_end)\n",
    "answer_end = torch.argmax(end_logits_positive)\n",
    "max_end_prob = output.end_logits[0][answer_end].item()\n",
    "sum_joint_prob = max_start_prob + max_end_prob\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "    \n",
    "print(\"\\nQuestion:\\n{}\".format(tokenizer.decode(question)))\n",
    "print(\"\\nAnswer:\\n{}.\".format(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f1e4a",
   "metadata": {},
   "source": [
    "### Output - For Report\n",
    "\n",
    "This section of text shows that span [49][50][51][52] is what we return. However, the section belows is what the ground truth says. SPan [51] is highlighted in red (we return 51, ground truth doesn't contain it).\n",
    "\n",
    "- 'About ten percent of customers visiting a DMV office do not bring what they need to complete their transaction, and have to come back a second time to finish their business. This can be as simple as not bringing sufficient funds to pay for a license renewal or not having the proof of auto insurance required to register a car. <font color='red'>Better yet ,</font> don t visit a DMV office at all, and see if your transaction can be performed online, like an address change, registration renewal, license renewal, replacing a lost title, paying a DRA or scheduling a road test. '\n",
    "- 'About ten percent of customers visiting a DMV office do not bring what they need to complete their transaction, and have to come back a second time to finish their business. This can be as simple as not bringing sufficient funds to pay for a license renewal or not having the proof of auto insurance required to register a car. don t visit a DMV office at all, and see if your transaction can be performed online, like an address change, registration renewal, license renewal, replacing a lost title, paying a DRA or scheduling a road test. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44f31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
